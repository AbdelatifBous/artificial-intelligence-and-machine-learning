{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layers Perceptron\n",
    "The goal of this notebook is to showcase how to code a multilayer perceptron in Python from scratch.\n",
    "It makes use of the Perceptron algorithm we developped in the perceptron jupyter notebook with modification to use backpropagation!\n",
    "\n",
    "We have two main class:\n",
    "- Neuron: Is used to model one neuron, most of the computation happens there\n",
    "- MultiLayerPerceptron: Is used to model the full neural networ. Here we only support fully connected neural network and in this particular notebook we only have one hidden layer and one neuron in the output layer.\n",
    "\n",
    "There is still much to be improved here, like modeling a layer so that it is easier to compose neural network, but the goal right now was to showcase how backpropagation works.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class Neuron():\n",
    "    '''\n",
    "        A conceptual Neuront hat can be trained using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, position_in_layer, is_output_neuron=False):\n",
    "        self.weights = []\n",
    "        self.inputs = []\n",
    "        self.output = None\n",
    "        \n",
    "        # This is used for the backpropagation update\n",
    "        self.updated_weights = []\n",
    "        # This is used to know how to update the weights\n",
    "        self.is_output_neuron = is_output_neuron\n",
    "        # This delta is used for the update at the backpropagation\n",
    "        self.delta = None\n",
    "        # This is used for the backpropagation update\n",
    "        self.position_in_layer = position_in_layer \n",
    "        \n",
    "    def attach_to_output(self, neurons):\n",
    "        '''\n",
    "            Helper function to store the reference of the other neurons\n",
    "            To this particular neuron (used for backpropagation)\n",
    "        '''\n",
    "        \n",
    "        self.output_neurons = []\n",
    "        for neuron in neurons:\n",
    "            self.output_neurons.append(neuron)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def init_weights(self, num_input):\n",
    "        '''\n",
    "            This is used to setup the weights when we know how many inputs there is for\n",
    "            a given neuron\n",
    "        '''\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_input):\n",
    "            self.weights.append(random.uniform(0,1))\n",
    "        \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Given a row of data it will predict what the output should be for\n",
    "            this given neuron. We can have many input, but only one output for a neuron\n",
    "        '''\n",
    "        \n",
    "        # Reset the inputs\n",
    "        self.inputs = []\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        activation = 0\n",
    "        for weight, feature in zip(self.weights, row):\n",
    "            self.inputs.append(feature)\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        \n",
    "        self.output = self.sigmoid(activation)\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "            \n",
    "    def update_neuron(self):\n",
    "        '''\n",
    "            Will update a given neuron weights by replacing the current weights\n",
    "            with those used during the backpropagation. This need to be done at the end of the\n",
    "            backpropagation\n",
    "        '''\n",
    "        \n",
    "        self.weights = []\n",
    "        for new_weight in self.updated_weights:\n",
    "            self.weights.append(new_weight)\n",
    "    \n",
    "    def calculate_update(self, learning_rate, target):\n",
    "        '''\n",
    "            This function will calculate the updated weights for this neuron. It will first calculate\n",
    "            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n",
    "            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n",
    "            for other update in the backpropagation algorithm.\n",
    "        '''\n",
    "        \n",
    "        if self.is_output_neuron:\n",
    "            # Calculate the delta for the output\n",
    "            self.delta = (self.output - target)*self.output*(1-self.output)\n",
    "        else:\n",
    "            # Calculate the delta\n",
    "            delta_sum = 0\n",
    "            # this is to know which weights this neuron is contributing in the output layer\n",
    "            cur_weight_index = self.position_in_layer \n",
    "            for output_neuron in self.output_neurons:\n",
    "                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n",
    "\n",
    "            # Update this neuron delta\n",
    "            self.delta = delta_sum*self.output*(1-self.output)\n",
    "            \n",
    "            \n",
    "        # Reset the update weights\n",
    "        self.updated_weights = []\n",
    "        \n",
    "        # Iterate over each weight and update them\n",
    "        for cur_weight, cur_input in zip(self.weights, self.inputs):\n",
    "            gradient = self.delta*cur_input\n",
    "            new_weight = cur_weight - learning_rate*gradient\n",
    "            self.updated_weights.append(new_weight)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "        We will be creating the multi-layer perceptron with only two layer:\n",
    "        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
    "    '''\n",
    "    def __init__(self, num_neuron, learning_rate = 0.01, num_iteration = 100):\n",
    "        # One output Neuron\n",
    "        self.output_neuron = Neuron(0, is_output_neuron=True)\n",
    "        \n",
    "        # One hidden layer\n",
    "        self.perceptrons = []\n",
    "        for i in range(num_neuron):\n",
    "            # Create neuron\n",
    "            neuron = Neuron(i)\n",
    "            # attach the output layer to this neuron\n",
    "            neuron.attach_to_output([self.output_neuron])\n",
    "            # Append to layer\n",
    "            self.perceptrons.append(neuron)\n",
    "        \n",
    "        # Training parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iteration = num_iteration\n",
    "        self.num_neuron = num_neuron\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Main training function of the neural network algorithm. This will make use of backpropagation.\n",
    "            It will use stochastic gradient descent by selecting one row at random from the dataset and \n",
    "            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n",
    "            Once all the new weights are calculated, the whole network weights will be updated\n",
    "        '''\n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Init the weights\n",
    "        for neuron in self.perceptrons:\n",
    "            neuron.init_weights(num_feature)\n",
    "        self.output_neuron.init_weights(len(self.perceptrons))\n",
    "\n",
    "        # Launch the training algorithm\n",
    "        for i in range(self.num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i] # take the random sample from the dataset\n",
    "            yhat = self.predict(row)\n",
    "            target = y[r_i]\n",
    "            \n",
    "            # Calculate update the last layer\n",
    "            self.output_neuron.calculate_update(self.learning_rate, target)\n",
    "            # Calculate update the hidden layer\n",
    "            for neuron in self.perceptrons:\n",
    "                neuron.calculate_update(self.learning_rate, target)\n",
    "            \n",
    "            # Update the last layer\n",
    "            self.output_neuron.update_neuron()\n",
    "            # Update the hidden layer\n",
    "            for neuron in self.perceptrons:\n",
    "                neuron.update_neuron()\n",
    "            \n",
    "            # At every 100 iteration we calculate the error\n",
    "            # on the whole training set\n",
    "            if i % 1000 == 0:\n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict(row)\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        \n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        \n",
    "        # Gather all the activation in the hidden layer\n",
    "        row.append(1) # need to add the bias\n",
    "        activations = [perceptron.predict(row) for perceptron in self.perceptrons]\n",
    "        \n",
    "        activations.append(1) # need to add the bias\n",
    "        activation = self.output_neuron.predict(activations)\n",
    "        \n",
    "        # Decide if we output a 1 or 0\n",
    "        if activation >= 0.5:\n",
    "            return 1.0\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = 0.5\n",
      "Iteration 1000 with error = 0.5\n",
      "Iteration 2000 with error = 0.5\n",
      "Iteration 3000 with error = 0.5\n",
      "Iteration 4000 with error = 0.5\n",
      "Iteration 5000 with error = 0.5\n",
      "Iteration 6000 with error = 0.25\n",
      "Iteration 7000 with error = 0.5\n",
      "Iteration 8000 with error = 0.5\n",
      "Iteration 9000 with error = 0.5\n",
      "Iteration 10000 with error = 0.5\n",
      "Iteration 11000 with error = 0.5\n",
      "Iteration 12000 with error = 0.25\n",
      "Iteration 13000 with error = 0.5\n",
      "Iteration 14000 with error = 0.75\n",
      "Iteration 15000 with error = 0.5\n",
      "Iteration 16000 with error = 0.25\n",
      "Iteration 17000 with error = 0.75\n",
      "Iteration 18000 with error = 0.5\n",
      "Iteration 19000 with error = 0.25\n",
      "Iteration 20000 with error = 0.25\n",
      "Iteration 21000 with error = 0.25\n",
      "Iteration 22000 with error = 0.5\n",
      "Iteration 23000 with error = 0.25\n",
      "Iteration 24000 with error = 0.25\n",
      "Iteration 25000 with error = 0.25\n",
      "Iteration 26000 with error = 0.5\n",
      "Iteration 27000 with error = 0.25\n",
      "Iteration 28000 with error = 0.5\n",
      "Iteration 29000 with error = 0.25\n",
      "Iteration 30000 with error = 0.25\n",
      "Iteration 31000 with error = 0.25\n",
      "Iteration 32000 with error = 0.25\n",
      "Iteration 33000 with error = 0.25\n",
      "Iteration 34000 with error = 0.25\n",
      "Iteration 35000 with error = 0.25\n",
      "Iteration 36000 with error = 0.25\n",
      "Iteration 37000 with error = 0.25\n",
      "Iteration 38000 with error = 0.25\n",
      "Iteration 39000 with error = 0.25\n",
      "Iteration 40000 with error = 0.75\n",
      "Iteration 41000 with error = 0.25\n",
      "Iteration 42000 with error = 0.25\n",
      "Iteration 43000 with error = 0.25\n",
      "Iteration 44000 with error = 0.25\n",
      "Iteration 45000 with error = 0.25\n",
      "Iteration 46000 with error = 0.25\n",
      "Iteration 47000 with error = 0.25\n",
      "Iteration 48000 with error = 0.25\n",
      "Iteration 49000 with error = 0.25\n",
      "Iteration 50000 with error = 0.25\n",
      "Iteration 51000 with error = 0.25\n",
      "Iteration 52000 with error = 0.25\n",
      "Iteration 53000 with error = 0.25\n",
      "Iteration 54000 with error = 0.25\n",
      "Iteration 55000 with error = 0.25\n",
      "Iteration 56000 with error = 0.25\n",
      "Iteration 57000 with error = 0.25\n",
      "Iteration 58000 with error = 0.25\n",
      "Iteration 59000 with error = 0.25\n",
      "Iteration 60000 with error = 0.25\n",
      "Iteration 61000 with error = 0.25\n",
      "Iteration 62000 with error = 0.25\n",
      "Iteration 63000 with error = 0.25\n",
      "Iteration 64000 with error = 0.25\n",
      "Iteration 65000 with error = 0.25\n",
      "Iteration 66000 with error = 0.25\n",
      "Iteration 67000 with error = 0.25\n",
      "Iteration 68000 with error = 0.25\n",
      "Iteration 69000 with error = 0.25\n",
      "Iteration 70000 with error = 0.25\n",
      "Iteration 71000 with error = 0.25\n",
      "Iteration 72000 with error = 0.25\n",
      "Iteration 73000 with error = 0.25\n",
      "Iteration 74000 with error = 0.25\n",
      "Iteration 75000 with error = 0.25\n",
      "Iteration 76000 with error = 0.25\n",
      "Iteration 77000 with error = 0.25\n",
      "Iteration 78000 with error = 0.0\n",
      "Iteration 79000 with error = 0.25\n",
      "Iteration 80000 with error = 0.25\n",
      "Iteration 81000 with error = 0.25\n",
      "Iteration 82000 with error = 0.25\n",
      "Iteration 83000 with error = 0.0\n",
      "Iteration 84000 with error = 0.25\n",
      "Iteration 85000 with error = 0.0\n",
      "Iteration 86000 with error = 0.25\n",
      "Iteration 87000 with error = 0.0\n",
      "Iteration 88000 with error = 0.0\n",
      "Iteration 89000 with error = 0.0\n",
      "Iteration 90000 with error = 0.25\n",
      "Iteration 91000 with error = 0.0\n",
      "Iteration 92000 with error = 0.0\n",
      "Iteration 93000 with error = 0.0\n",
      "Iteration 94000 with error = 0.0\n",
      "Iteration 95000 with error = 0.0\n",
      "Iteration 96000 with error = 0.0\n",
      "Iteration 97000 with error = 0.0\n",
      "Iteration 98000 with error = 0.0\n",
      "Iteration 99000 with error = 0.0\n",
      "Iteration 100000 with error = 0.0\n",
      "Iteration 101000 with error = 0.0\n",
      "Iteration 102000 with error = 0.0\n",
      "Iteration 103000 with error = 0.0\n",
      "Iteration 104000 with error = 0.0\n",
      "Iteration 105000 with error = 0.0\n",
      "Iteration 106000 with error = 0.0\n",
      "Iteration 107000 with error = 0.0\n",
      "Iteration 108000 with error = 0.0\n",
      "Iteration 109000 with error = 0.0\n",
      "Iteration 110000 with error = 0.0\n",
      "Iteration 111000 with error = 0.0\n",
      "Iteration 112000 with error = 0.0\n",
      "Iteration 113000 with error = 0.0\n",
      "Iteration 114000 with error = 0.0\n",
      "Iteration 115000 with error = 0.0\n",
      "Iteration 116000 with error = 0.0\n",
      "Iteration 117000 with error = 0.0\n",
      "Iteration 118000 with error = 0.0\n",
      "Iteration 119000 with error = 0.0\n",
      "Iteration 120000 with error = 0.0\n",
      "Iteration 121000 with error = 0.0\n",
      "Iteration 122000 with error = 0.0\n",
      "Iteration 123000 with error = 0.0\n",
      "Iteration 124000 with error = 0.0\n",
      "Iteration 125000 with error = 0.0\n",
      "Iteration 126000 with error = 0.0\n",
      "Iteration 127000 with error = 0.0\n",
      "Iteration 128000 with error = 0.0\n",
      "Iteration 129000 with error = 0.0\n",
      "Iteration 130000 with error = 0.0\n",
      "Iteration 131000 with error = 0.0\n",
      "Iteration 132000 with error = 0.0\n",
      "Iteration 133000 with error = 0.0\n",
      "Iteration 134000 with error = 0.0\n",
      "Iteration 135000 with error = 0.0\n",
      "Iteration 136000 with error = 0.0\n",
      "Iteration 137000 with error = 0.0\n",
      "Iteration 138000 with error = 0.0\n",
      "Iteration 139000 with error = 0.0\n",
      "Iteration 140000 with error = 0.0\n",
      "Iteration 141000 with error = 0.0\n",
      "Iteration 142000 with error = 0.0\n",
      "Iteration 143000 with error = 0.0\n",
      "Iteration 144000 with error = 0.0\n",
      "Iteration 145000 with error = 0.0\n",
      "Iteration 146000 with error = 0.0\n",
      "Iteration 147000 with error = 0.0\n",
      "Iteration 148000 with error = 0.0\n",
      "Iteration 149000 with error = 0.0\n"
     ]
    }
   ],
   "source": [
    "# XOR function (one or the other but not both)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "num_neuron = 10\n",
    "num_iteration = 150000\n",
    "clf = MultiLayerPerceptron(num_neuron, num_iteration = num_iteration)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
