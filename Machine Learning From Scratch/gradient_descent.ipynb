{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "In this notebook we will code the Gradient Descent Algorithm from Scratch using Python and we will visualize how it behaves when given a simple learning task.\n",
    "\n",
    "This algorithm estimate the gradient using a the training data to calculate the gradient\n",
    "It needs a parameter 'a' called the learning rate\n",
    "\n",
    "This algorithm doesn't scale well with lots of training instance since it calculate the gradient on all the data points, its variant the Stochastic Gradient Descent which estimate the gradient from 1 point is less computationally restrictive.\n",
    "\n",
    "# Pseudocode\n",
    "- initialize the parameters w randomly and select a learning rate (a)\n",
    "- While a minima is not found\n",
    "    - Calculate the gradient using all the data\n",
    "    - w := w - a*gradient\n",
    "        \n",
    "For the update if we have multiple feature we need to take the partial derivative of each feature for the function we are trying to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(x) = w1 + w2*x\n",
    "# we are trying to fit the best w1 and w2 we can on the dataset\n",
    "# (x1,x2,...,xn) with (y1,y2,...,yn)\n",
    "# we are using square error\n",
    "# We need to minimize: Sum[i=1:N](yhat_i - yi)^2\n",
    "# Which tranlsate to Sum[i=1:N](w1 + w2*xi - yi)^2\n",
    "\n",
    "# The gradient are then the following\n",
    "# df(x)/d(w1) = (1/N) * (Sum[i=1:N] 2*(w1 + w2*xi - yi))\n",
    "# df(x)/d(w2) = (1/N) * (Sum[i=1:N] 2*xi*(w1 + w2*xi - yi))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(w1,w2,x):\n",
    "    '''\n",
    "        f: function we are trying to estimate the parameters (line)\n",
    "        w1: bias\n",
    "        w2: slope\n",
    "        x: a point in the plane\n",
    "        \n",
    "        return yhat an estimate of y\n",
    "    '''\n",
    "    yhat = w1 + w2*x\n",
    "    return yhat\n",
    "\n",
    "def dx_w1(w1,w2,x,y):\n",
    "    '''\n",
    "        dx_w1: partial derivative of the weight w1 for function f\n",
    "        w1: bias\n",
    "        w2: slope\n",
    "        x: a point in the plane\n",
    "        y: the response of the point x\n",
    "        \n",
    "        return gradient which is the gradient at that point for this x and y for w1\n",
    "    '''\n",
    "    yhat = f(w1,w2,x)\n",
    "    gradient = 2*(yhat - y)\n",
    "    return gradient\n",
    "\n",
    "def dx_w2(w1,w2,x,y):\n",
    "    '''\n",
    "        dx_w2: partial derivative of the weight w2 for function f\n",
    "        w1: bias\n",
    "        w2: slope\n",
    "        x: a point in the plane\n",
    "        y: the response of the point x\n",
    "        \n",
    "        return gradient which is the gradient at that point for this x and y for w2\n",
    "    '''    \n",
    "    yhat = f(w1,w2,x)\n",
    "    gradient = 2*x*(yhat - y)\n",
    "    return gradient\n",
    "\n",
    "def gradient_w1(w1,w2,xs,ys):\n",
    "    '''\n",
    "        gradient_w1: estimate mean gradient over all point for w1\n",
    "        w1: bias\n",
    "        w2: slope\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return gradient which is the gradient at that point for all x and y for w1\n",
    "    '''        \n",
    "    N = len(ys)\n",
    "    \n",
    "    total = 0\n",
    "    for x,y in zip(xs,ys):\n",
    "        total = total + dx_w1(w1,w2,x,y)\n",
    "    \n",
    "    gradient = total/N\n",
    "    return gradient\n",
    "\n",
    "def gradient_w2(w1,w2,xs,ys):\n",
    "    '''\n",
    "        gradient_w2: estimate mean gradient over all point for w2\n",
    "        w1: bias\n",
    "        w2: slope\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return gradient which is the gradient at that point for all x and y for w2\n",
    "    '''            \n",
    "    N = len(ys)\n",
    "    \n",
    "    total = 0\n",
    "    for x,y in zip(xs,ys):\n",
    "        total = total + dx_w2(w1,w2,x,y)\n",
    "    \n",
    "    gradient = total/N\n",
    "    return gradient\n",
    "\n",
    "def gradient_descent(xs, ys, learning_rate = 0.01, max_num_iteration = 1000):\n",
    "    '''\n",
    "        gradient_descent: will estimate the parameters w1 and w2 (here it uses least square cost function)\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        learning_rate: the learning rate for the step that weights update will take\n",
    "        max_num_iteration: the number of iteration before we stop updating\n",
    "        \n",
    "        return w1 and w2 which is the bias and the slope of the formula\n",
    "    '''    \n",
    "    # Randomly initialize the weight w1 and w2\n",
    "    w1 = np.random.uniform(0,1,1)\n",
    "    w2 = np.random.uniform(0,1,1)\n",
    "    \n",
    "    for i in range(max_num_iteration):\n",
    "        w1 = w1 - learning_rate*gradient_w1(w1,w2,xs,ys)\n",
    "        w2 = w2 - learning_rate*gradient_w2(w1,w2,xs,ys)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Iteration {i}\")\n",
    "            print(f\"W1 = {w1}\")\n",
    "            print(f\"W2 = {w2}\")\n",
    "    \n",
    "    return (w1,w2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "W1 = [0.62195325]\n",
      "W2 = [0.58668646]\n",
      "Iteration 100\n",
      "W1 = [0.45896804]\n",
      "W2 = [0.90766336]\n",
      "Iteration 200\n",
      "W1 = [0.31034397]\n",
      "W2 = [0.93756402]\n",
      "Iteration 300\n",
      "W1 = [0.20984768]\n",
      "W2 = [0.95778218]\n",
      "Iteration 400\n",
      "W1 = [0.14189433]\n",
      "W2 = [0.97145325]\n",
      "Iteration 500\n",
      "W1 = [0.09594578]\n",
      "W2 = [0.98069732]\n",
      "Iteration 600\n",
      "W1 = [0.0648764]\n",
      "W2 = [0.98694796]\n",
      "Iteration 700\n",
      "W1 = [0.04386798]\n",
      "W2 = [0.9911745]\n",
      "Iteration 800\n",
      "W1 = [0.02966255]\n",
      "W2 = [0.99403239]\n",
      "Iteration 900\n",
      "W1 = [0.02005715]\n",
      "W2 = [0.99596484]\n",
      "[0.01361537] [0.99726082]\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 1\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [1,2,3,4,5,6,7]\n",
    "(w1,w2) = gradient_descent(xs,ys)\n",
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "W1 = [0.48068551]\n",
      "W2 = [1.04949241]\n",
      "Iteration 100\n",
      "W1 = [0.43884588]\n",
      "W2 = [1.9117116]\n",
      "Iteration 200\n",
      "W1 = [0.29673781]\n",
      "W2 = [1.94030135]\n",
      "Iteration 300\n",
      "W1 = [0.2006475]\n",
      "W2 = [1.9596331]\n",
      "Iteration 400\n",
      "W1 = [0.13567337]\n",
      "W2 = [1.9727048]\n",
      "Iteration 500\n",
      "W1 = [0.09173931]\n",
      "W2 = [1.9815436]\n",
      "Iteration 600\n",
      "W1 = [0.06203208]\n",
      "W2 = [1.98752019]\n",
      "Iteration 700\n",
      "W1 = [0.04194471]\n",
      "W2 = [1.99156143]\n",
      "Iteration 800\n",
      "W1 = [0.02836208]\n",
      "W2 = [1.99429403]\n",
      "Iteration 900\n",
      "W1 = [0.01917781]\n",
      "W2 = [1.99614175]\n",
      "[0.01301845] [1.99738091]\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 0 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [2,4,6,8,10,12,14]\n",
    "(w1,w2) = gradient_descent(xs,ys)\n",
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "W1 = [0.48490318]\n",
      "W2 = [1.21010945]\n",
      "Iteration 100\n",
      "W1 = [0.77081465]\n",
      "W2 = [2.04610823]\n",
      "Iteration 200\n",
      "W1 = [0.84502997]\n",
      "W2 = [2.03117736]\n",
      "Iteration 300\n",
      "W1 = [0.89521272]\n",
      "W2 = [2.02108144]\n",
      "Iteration 400\n",
      "W1 = [0.92914517]\n",
      "W2 = [2.0142548]\n",
      "Iteration 500\n",
      "W1 = [0.95208955]\n",
      "W2 = [2.00963878]\n",
      "Iteration 600\n",
      "W1 = [0.96760402]\n",
      "W2 = [2.00651753]\n",
      "Iteration 700\n",
      "W1 = [0.97809456]\n",
      "W2 = [2.00440701]\n",
      "Iteration 800\n",
      "W1 = [0.98518803]\n",
      "W2 = [2.00297992]\n",
      "Iteration 900\n",
      "W1 = [0.98998447]\n",
      "W2 = [2.00201495]\n",
      "[0.99320117] [2.00136781]\n"
     ]
    }
   ],
   "source": [
    "# Here we have a simple line with intercept = 1 and slope = 2\n",
    "xs = [1,2,3,4,5,6,7]\n",
    "ys = [3,5,7,9,11,13,15]\n",
    "(w1,w2) = gradient_descent(xs,ys)\n",
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
